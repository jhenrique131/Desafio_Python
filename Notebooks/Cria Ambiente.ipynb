{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "065ef600-3144-4773-908e-754a8cc67273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criar os diret√≥rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bba2c38-06ce-40e9-8dbb-308baa982cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configura√ß√µes iniciais\n",
    "catalog = \"workspace\"\n",
    "schemas = [\"processar\", \"processado\", \"bronze\", \"silver\", \"gold\"]\n",
    "volume = \"vendas\"\n",
    "# Criar cat√°logo (se n√£o existir)\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "# Criar schemas e volumes dentro do cat√°logo\n",
    "for schema in schemas:\n",
    "    # Criar schema\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    " \n",
    "    # Criar volume\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\")\n",
    " \n",
    "    print(f\"Volume criado: {catalog}.{schema}.{volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "add4d984-57e3-4953-9482-77e74e7a69a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import requests\n",
    "from pyspark.dbutils import DBUtils\n",
    "# Configura√ß√µes\n",
    "url = \"https://github.com/andrerosa77/trn-pyspark/raw/main/dados_2012.csv\"\n",
    "final_path = \"/Volumes/workspace/processar/vendas/dados_2012.csv\"\n",
    "temp_path = \"/Volumes/workspace/temp/vendas/dados_2012.csv\"\n",
    "# Inicializar DBUtils\n",
    "dbutils = DBUtils(spark)\n",
    "# 1. Criar volume tempor√°rio (se n√£o existir)\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS workspace\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.temp\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS workspace.temp.vendas\")\n",
    " \n",
    "print(\"Volume tempor√°rio criado: workspace.temp.vendas\")\n",
    "# 2. Fazer download do arquivo\n",
    "print(\"üì• Fazendo download do arquivo...\")\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "# 3. Salvar conte√∫do diretamente no volume tempor√°rio\n",
    "content_str = response.content.decode('utf-8')\n",
    "dbutils.fs.put(temp_path, content_str, overwrite=True)\n",
    " \n",
    "print(f\"Arquivo tempor√°rio salvo em: {temp_path}\")\n",
    "# 4. Mover para o destino final\n",
    "dbutils.fs.mv(temp_path, final_path)\n",
    "print(f\"Arquivo movido para destino final: {final_path}\")\n",
    "# 5. Limpar arquivo tempor√°rio (opcional)\n",
    "dbutils.fs.rm(temp_path)\n",
    "print(\"üßπ Arquivo tempor√°rio removido\")\n",
    "# 6. Verificar conte√∫do do volume de destino\n",
    "print(\"\\nüìÇ Conte√∫do do volume de destino:\")\n",
    "display(dbutils.fs.ls(\"/Volumes/workspace/processar/vendas\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cria Ambiente",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
